% Numerical Computing II
% Homework 22
% Initial Value Problems for ODE
% May 2, 2010
% 22.2, 22.3, 22.5, 22.6, 22.8

\documentclass[11pt]{article}
\usepackage{listings}
\usepackage[fleqn]{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{amsthm}
\addtolength{\textwidth}{2in}
\addtolength{\oddsidemargin}{-1in}
\addtolength{\evensidemargin}{1in}
\begin{document}         
% Start your text
\newcommand{\makehomework}[2]%
{\begin{center}%
	\Huge #1\\%
	\Large #2\\%
	Marty Fuhry\\%
	\today%
\end{center}}
\makehomework{Numerical Computing II}{Homework 22: Initial Value Problems for Ordinary Differential Equations}

\lstset{language=Matlab,numbers=left,frame=single,breaklines=true,morecomment=[l]{//}}
\section*{Exercise 22.2}
To show that the Backwards Euler's Method is $O(h)$, we begin by showing the following:
\begin{flalign*}
    u_n &= (\frac{1}{1-\lambda h})^n u_0\\
        &= (1 + \lambda h + O(h^2))^n u_0\\
    e^{\lambda nh + n O(h^2)}u_0 &= (1 + \lambda h + O(h^2))^nu_0.
\end{flalign*}
So we have equality with $O(h^2)$ between $u_n$ and $e^{n \lambda h + n O(h^2)}$.
Use this equality to solve the global error. Note that $t_n - t_0 = nh$.
\begin{flalign*}
    u_n - u(t_n) &= u_n - u_0e^{\lambda(t_n - t_0)}\\
                 &= u_n - u_0e^{\lambda nh}\\
                 &= u_0 e^{\lambda nh + n O(h^2)} - u_0 e^{\lambda nh}.
\end{flalign*}
We know that $nh = t_n - t_0$ is a constant, so $n O(h^2) = O(nh^2) = (t_n - t_0) O(h) = O(h)$. Then,
\begin{align*}
    e^{n O(h^2)} = e^{O(h)} = 1 + O(h).
\end{align*}
Substituting this in for our global error yields:
\begin{flalign*}
    u_n - u(t_n) &= u_0 e^{\lambda nh} e^{n O(h^2)} - u_0 e^{\lambda nh}\\
                 &= u_0 e^{\lambda nh} e^{O(h)}     - u_0 e^{\lambda nh}\\
                 &= u_0 (1 + O(h)) e^{\lambda nh} - u_0 e^{\lambda nh}\\
                 &= u_0 e ^{\lambda nh} + u_0 O(h) - u_0 e^{\lambda nh}\\
                 &= u_0 O(h)\\
                 &= O(h)
\end{flalign*}
\qedsymbol

\section*{Exercise 22.3}
We need to explicitly solve for $u_1$ in the trapezoidal method $u_1 = 1/2(u_0 + hf(t_0,u_0) + u_1 + hf(t_1,u_1))$.
Multiplying this out and solving $u_1$ gives us:
\begin{flalign*}
    u_1 &= u_0 + \frac{h}{2} (f(t_0,u_0) + f(t_1,u_1))\\
        &= u_0 + \frac{h}{2} (\lambda u_0 + \lambda u_1)\\
        &= u_0 + \frac{h\lambda}{2} u_0 + \frac{h\lambda}{2} u_1\\
    u_1 - \frac{h\lambda}{2} u_1 &= u_0 + \frac{h\lambda}{2}u_0\\
    u_1 ( 1 - \frac{h\lambda}{2}) &= u_0(1 + \frac{h\lambda}{2})\\
    u_1 &= u_0\frac{1 + \frac{h\lambda}{2}}{1 - \frac{h\lambda}{2}}.
\end{flalign*}
Now, we need to manipulate $u_1$ by solving it as a geometric series:
\begin{flalign*}
    u_1 &= u_0(1 + \frac{h\lambda}{2})(\frac{1}{1 - h\lambda})\\
        &= u_0(1 + \frac{h\lambda}{2})(1 + \frac{h\lambda}{2} + (\frac{h\lambda}{2})^2 + (\frac{h\lambda}{2})^3 + O(h^4))\\
        &= u_0(1 + h\lambda + \frac{(h\lambda)^2}{2} + \frac{2(h\lambda)^3}{8} + \frac{(h\lambda)^3}{8} + O(h^4))\\
        &= u_0(1 + h\lambda + \frac{(h\lambda)^2}{2} + \frac{3(h\lambda)^3}{8} + O(h^4)).
\end{flalign*}
Now, using the exponential expansion:
\begin{align*}
    u_0e^{h\lambda} = u_0(1 + h\lambda + \frac{(h\lambda)^2}{2} + \frac{(h \lambda)^3}{6} + O(h^4))
\end{align*}
we can finally solve the local error:
\begin{flalign*}
    u_1 - u(t_1) =& u_0(1 + h\lambda + \frac{(h\lambda)^2}{2} + \frac{3(h\lambda)^3}{8} + O(h^4)) -\\
                  & u_0(1 + h\lambda + \frac{(h\lambda)^2}{2} + \frac{(h \lambda)^3}{6} + O(h^4))\\
                 =& O(h^3).
\end{flalign*}
\qedsymbol
\section*{Exercise 22.5}
\begin{align*}
    u_{j+1} = u_j + h(\alpha u_{j+1}' + \beta u_j')
\end{align*}
We use this method with the conditions $u_l' = f(t_l,u_l)$.
We begin with the function $u(t) = 1$, which means that $f(t_l, u_l) = 0 \forall t_i, u_i$.
Choose $t_0 = 0, t_1 = h$ and then $u(t_0) = 0, u(t_1) = 0$.
\begin{flalign*}
    u_0' &= f(t_0, u_0) = 0\\
    u_1' &= f(t_1, u_1) = 0\\
    u_1  &= u_0 + h(\alpha u_1' + \beta u_0')\\
         &= u_0 + h(\alpha 0    + \beta 0   )\\
    u_1  &= u_0 = 0.
\end{flalign*}
Now we try $u(t) = t$, which means that $f(t_l, u_l) = 1 \forall t_i, u_i$.
Choose $t_0 = 0, t_1 = h$ and then $u(t_0) = 0, u(t_1) = h$.
\begin{flalign*}
    u_0' &= f(t_0, u_0) = 1\\
    u_1' &= f(t_1, u_1) = 1\\
    u_1  &= u_0 + h(\alpha f(t_1, u_1) + \beta f(t_0, u_0))\\
         &= u_0 + h(\alpha 1    + \beta 1   )\\
    h    &= h(\alpha + \beta)\\
    1    &= \alpha + \beta
\end{flalign*}
We have our first equation, then. Now we try $u(t) = t^2$, which means that $f(t_l, u_l) = 2t$.
Choose $t_0 = 0, t_1 = h$ and then $u(t_0) = 0, u(t_1) = h^2$.
\begin{flalign*}
    u_0' &= f(t_0, u_0) = 0\\
    u_1' &= f(t_1, u_1) = 2h\\
    u_1  &= u_0 + h(\alpha u_1' +  \beta u_0')\\
    h^2  &= 0   + h(\alpha 2h   + \beta 0)\\
    h^2  &= \alpha 2h^2\\
    \frac{1}{2} &= \alpha = \beta.
\end{flalign*}
Then, try $u(t) = t^3$, which means that $f(t_l, u_l) = 3t^2$.
Choose $t_0 = 0, t_1 = h$ and then $u(t_0) = 0, u(t_1) = h^3$.
\begin{flalign*}
    u_0' &= f(t_0, u_0) = 0\\
    u_1' &= f(t_1, u_1) = 3h^2\\
    u_1  &= u_0 + h(\alpha u_1' +  \beta u_0')\\
    h^3  &= 0   + h(\alpha 3h^2   + \beta 0)\\
    h^3  &= \alpha 3h^3\\
    \frac{1}{3} &= \alpha \implies \beta = \frac{2}{3}. 
\end{flalign*}
But this doesn't agree with our other equations. 
$\alpha = \beta = 1/2$ works for all polynomials up to and including $t^2$.
When we reach the polynomial $t^3$, we need $\alpha = 1/3$, which conflicts for polynomials of lower degree.
\section*{Exercise 22.6}

Apply each method to:
\begin{align*}
    u'(t) = -25u(t)\\
    t\geq t_0\\
    u(t_0)=u_0.
\end{align*}
\subsubsection*{Trapezoidal Method}
The trapezoidal method is given by:
\begin{align*}
    u_{j+1} = u_j + \frac{h}{2}(f(t_j, u_j) + f(t_{j+1}, u_{j+1})).
\end{align*}
We apply this rule for the function $f(t,u) = \lambda u$ and observe:
\begin{flalign*}
    u_{j+1} &= u_j + \frac{h}{2}(f(t_j, u_j) + f(t_{j+1}, u_{j+1}))\\
            &= u_j + \frac{h}{2}(\lambda u_j + \lambda u_{j+1})\\
            &= u_j + \frac{h}{2}\lambda u_j  + \frac{h}{2}\lambda u_{j+1})\\
    u_{j+1} -\frac{h}{2} \lambda u_{j+1} &= u_j + \frac{h}{2}\lambda u_j\\ 
    u_{j+1} (1 - \frac{h}{2} \lambda) &=    u_j + \frac{h}{2}\lambda u_j\\ 
    u_{j+1} &=    u_j \frac{1 + \frac{h}{2}\lambda}{1 - \frac{h}{2} \lambda} \\ 
    u_{j+1} &=    u_0 (\frac{1 + \frac{h}{2}\lambda}{1 - \frac{h}{2} \lambda})^j.
\end{flalign*}
This converges if and only if
\begin{equation}
    \label{eqn:converge}
    \frac{1 + \frac{h}{2}\lambda}{1 - \frac{h}{2}\lambda} < 1.
\end{equation}
Since $h > 0$ and $\lambda < 0$, (\ref{eqn:converge}) is equivalent to
\begin{align*}
    1 + \frac{h}{2} \lambda < 1 - \frac{h}{2} \lambda\\
    h \lambda < 0\\
    -25 h < 0.
\end{align*}
In this case, it works for all positive $h$.

\subsubsection*{Heun's Method}
Heun's method is given by:
\begin{align*}
    u_{j+1} = u_j + \frac{h}{2}(f(t_j,u_j) + f(t_{j+1}, \hat{u_{j+1}})).
\end{align*}
Note that $\hat{u_{j+1}}$ is taken from Euler's method
\begin{align*}
    \hat{u_{j+1}} = u_j + hf(t_j,u_j). 
\end{align*}
Rewriting Heun's method, we get:
\begin{align*}
    u_{j+1} = u_j + \frac{h}{2}(f(t_j,u_j) + f(t_{j+1}, u_j + hf(t_j,u_j))).
\end{align*}
We apply this rule for our function given by $f(t,u) = \lambda u$ and observe:
\begin{flalign*}
    u_{j+1} &= u_j + \frac{h}{2}(f(t_j,u_j) + f(t_{j+1}, u_j + hf(t_j,u_j)))\\
            &= u_j + \frac{h}{2}(f(t_j,u_j) + f(t_{j+1}, u_j + h \lambda u_j))\\
            &= u_j + \frac{h}{2}(\lambda u_j + \lambda (u_j + h \lambda u_j))\\
            &= u_j + \frac{h}{2}(2 \lambda u_j + h \lambda^2 u_j))\\
            &= u_j + h \lambda u_j + \frac{h^2}{2} \lambda^2 u_j))\\
            &= u_j(1  + h \lambda + \frac{h^2}{2} \lambda^2)\\
            &= u_0(1  + h \lambda + \frac{h^2}{2} \lambda^2)^j.
\end{flalign*}
This converges if and only if
\begin{flalign*}
     1  + h \lambda + \frac{h^2}{2} \lambda^2 <& 1\\
     2h \lambda + h^2 \lambda^2 <& 0\\
     h\lambda(2 + h\lambda) <& 0.
\end{flalign*}
Since $h \lambda < 0$, we need $2 + h \lambda > 0$ to satisfy the inequality. Then,
\begin{flalign*}
     2 + h\lambda >& 0\\
     h \lambda > -2\\
     h < \frac{2}{25}.
\end{flalign*}
We get this from $\lambda = -25$. So, when $h < \frac{2}{25}$, Heun's method converges in this case.

\subsubsection*{Example 22.7: Explicit Multistep Method}
The Explicit Multistep Method from Example 22.7 is given by
\begin{align*}
    u_{j+1} = u_j + h(\alpha u_j' + \beta u_{j-1}').
\end{align*}
Use the fact that $u_l' = f(t_l, u_l) = \lambda u_l$, and apply this rule for $f(t,u) = \lambda u$.
\begin{flalign*}
    u_{j+1} &= u_j + h(\alpha u_j' + \beta u_{j-1}')\\
            &= u_j + h(\alpha \lambda u_j + \beta \lambda u_{j-1})\\
            &= u_j + h\alpha \lambda u_j + h\beta \lambda u_{j-1}.
\end{flalign*}
We can't solve explicitly for this function since it is implicit, but we can use our knowledge of the values
of $u_j$ and $u_{j-1}$ to show that since $\lambda = -25$, we have a decreasing function. 
Therefore, $u_j < u_{j-1}$. Now, we have an upper bound on our $u_{j+1}$ value.
\begin{flalign*}
    u_{j+1} &= u_j + h\alpha \lambda u_j + h\beta \lambda u_{j-1}\\
            &<  u_{j-1} (1 + h \alpha \lambda + h \beta \lambda).
\end{flalign*}
This inequality is equal to
\begin{align*}
    u_0 ( 1 + h\alpha \lambda + h \beta \lambda)^{j-1}
\end{align*}
which converges if and only if
\begin{flalign*}
    1 + h\alpha \lambda + h \beta \lambda) &< 1\\
    h\alpha \lambda + h \beta \lambda) &< 0\\
    h\lambda(\alpha + \beta) &< 0\\
    h(\alpha + \beta) &> 0.
\end{flalign*}
Notice the inequality was flipped since $\lambda = - 25$. So this converges if $h$ is positive and the 
sum of the coefficients $\alpha + \beta$ is positive.

\subsubsection*{Conclusions}
The only method which required $h$ to be small was Heun's Method. Both the Trapezoidal Method and 
the Explicit Multistep Method from Example 22.7 converged for all positive values of $h$ (so long 
as the sum of the coefficients $\alpha + \beta$ was positive for the Explicit Multistep Method).
Therefore, both the Trapezoidal Method and the Explicit Multistep Method allowed the largest step length
by not setting any bounds on the size of $h$.

\section*{Exercise 22.8}
\begin{equation}
    \label{228:u}
    u'' - \epsilon (1 - u^2)u' + u = 0
\end{equation}
We begin by setting these up into a system of linear ordinary differential equations. Let $v = u'$, making 
$v' = u''$. Then,
\begin{equation}
    \label{228:v}
    v' = \epsilon(1 - u^2)v - u.
\end{equation}
We can build the following system  of equations based on (\ref{228:u}) and (\ref{228:v}):
\begin{flalign*}
    \frac{d}{dt}\left[\begin{matrix}
        u\\v
        \end{matrix}\right]
        =
        \left[\begin{matrix}
        v\\ \epsilon(1 - u^2)v - u
        \end{matrix}\right]
       = \left[\begin{matrix}
       0 & 1\\
       -1 & \epsilon(1 - u^2)
       \end{matrix}\right]
       \left[\begin{matrix}
       u\\v
       \end{matrix}\right]
\end{flalign*}
The following code can be used to model the system of ordinary differential equations.
% Import Program 
\lstinputlisting{f228.m}
\subsubsection*{ODE Solver \emph{lsode}}
We can use the \verb|lsode| solver in Octave to solve the following function with $\epsilon = 1$.
Run the following commands.
\begin{verbatim}
> y0 = [1/2 1/2]';
> t  = linspace(0,25,500);
> y  = lsode("f", y0, t);
\end{verbatim}
See the final pages for the graphs this method produced.
Note the regularity of these graphs, especially for when $\epsilon = .1 $ and $\epsilon = .01$.
The graphs are very normal without any "weird" nonstandard curves produced by the other methods.

\subsubsection*{ODE Solver \emph{ode23}}
The \verb|ode23| ODE Solver worked very well and produced a much more interesting set of graphs
than the \verb|lsode| solver produced.
The graphs for when $\epsilon = 1$ look nearly identical, but for the other two cases, we have a 
very neat looking start conforming to a diverging (or so it seems) pattern in the oscillations.
\subsubsection*{ODE Solver \emph{ode45}}
This method produced perhaps the "second best looking" graphs next to the uniformly beautiful graphs
that \verb|lsode| produced.
These graphs still have sharp cusps, but for the most part they look neat and quite good without
much bizarre oscillations like in \verb|ode23|.

\subsubsection*{ODE Solver \emph{ode78}}
Since Octave does not have a \verb|ode15| method (or at least, doesn't have one in the standard 
odesolver package), I chose to use \verb|ode78| as a fourth example.
This method produces the sharpest graphs of any of the solvers discussed here.
Perhaps simply being the "highest order" solver of the bunch doesn't mean that it will produce
the best looking graphs.

\subsubsection*{Conclusions}
The fastest solvers were probably \verb|ode45| and \verb|lsode|. The slowest solver may have been
\verb|ode23|, though the computed examples here aren't quite big enough to provide a meaningful
insight into the time complexity of the ODE Solvers' algorithms.
In Octave's help files, I read that \verb|lsode| has both a relative and absolute tolerance of 1.49$\times 10 ^ {-8}$
and uses a stiff integration method.
The rest of the solvers, \verb|ode45|, \verb|ode23|, and \verb|ode78| all use a relative tolerance of $10^{-3}$.

\begin{center}
\begin{figure}[h]
    \subfloat[\emph{lsode}]{
    \includegraphics[scale=0.4]{problem_228_eps1_graph.png}}
    \subfloat[\emph{ode23}]{
    \includegraphics[scale=0.4]{ode23_1.png}}

    \subfloat[\emph{ode45}]{
    \includegraphics[scale=0.4]{ode45_1.png}}
    \subfloat[\emph{ode78}]{
    \includegraphics[scale=0.4]{ode78_1.png}}
    \caption{Graph of (\ref{228:v}) with $\epsilon = 1$}
\end{figure}
\end{center}

\begin{center}
\begin{figure}[h]
    \subfloat[\emph{lsode}]{
    \includegraphics[scale=0.4]{problem_228_eps01_graph.png}}
    \subfloat[\emph{ode23}]{
    \includegraphics[scale=0.4]{ode23_01.png}}

    \subfloat[\emph{ode45}]{
    \includegraphics[scale=0.4]{ode45_01.png}}
    \subfloat[\emph{ode78}]{
    \includegraphics[scale=0.4]{ode78_01.png}}
    \caption{Graph of (\ref{228:v}) with $\epsilon = .1$}
\end{figure}
\end{center}

\begin{center}
\begin{figure}[h]
    \subfloat[\emph{lsode}]{
    \includegraphics[scale=0.4]{problem_228_eps001_graph.png}}
    \subfloat[\emph{ode23}]{
    \includegraphics[scale=0.4]{ode23_001.png}}

    \subfloat[\emph{ode45}]{
    \includegraphics[scale=0.4]{ode45_001.png}}
    \subfloat[\emph{ode78}]{
    \includegraphics[scale=0.4]{ode78_001.png}}
    \caption{Graph of (\ref{228:v}) with $\epsilon = .01$}
\end{figure}
\end{center}

% Import Program 
%\lstinputlisting{problem_17_1.m}

% Import Graph
%\begin{center}
%\includegraphics[scale=0.5]{problem_17_1_graph.png}
%\end{center}

% Stop your text
\end{document}
